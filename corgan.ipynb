{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Correlation device is a simple neural network that takes a signal as input and outputs a probability distribution\n",
    "class CorrelationDevice(nn.Module):\n",
    "    def __init__(self, signal_dim):\n",
    "        super(CorrelationDevice, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(signal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, signal_dim),\n",
    "            nn.Softmax(dim=-1)  # Assuming a categorical signal\n",
    "        )\n",
    "        \n",
    "    def forward(self, signal):\n",
    "        return self.main(signal)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, signal_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + signal_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Tanh()  # Assuming the output is a normalized image\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, signal):\n",
    "        combined_input = torch.cat((noise, signal), 1)\n",
    "        return self.main(combined_input)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, signal_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + signal_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output probability of being real\n",
    "        )\n",
    "        \n",
    "    def forward(self, sample, signal):\n",
    "        combined_input = torch.cat((sample, signal), 1)\n",
    "        return self.main(combined_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Parameters\n",
    "batch_size = 64\n",
    "signal_dim = 10\n",
    "noise_dim = 100  # size of the g's input noise vector\n",
    "image_dim = 28*28  # flatten MNIST\n",
    "learning_rate = 0.0002\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize MNIST \n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "C = CorrelationDevice(signal_dim=signal_dim)\n",
    "G = Generator(input_dim=noise_dim, signal_dim=signal_dim, output_dim=image_dim)\n",
    "D = Discriminator(input_dim=image_dim, signal_dim=signal_dim)\n",
    "\n",
    "optimizer_C = optim.Adam(C.parameters(), lr=learning_rate)\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/938], D Loss: 0.7140, G Loss: 0.9572, C Loss: -0.6027\n",
      "Epoch [1/50], Step [200/938], D Loss: 0.3417, G Loss: 1.3925, C Loss: -0.3253\n",
      "Epoch [1/50], Step [300/938], D Loss: 0.5476, G Loss: 3.6957, C Loss: -0.2905\n",
      "Epoch [1/50], Step [400/938], D Loss: 0.0054, G Loss: 9.1085, C Loss: -0.0010\n",
      "Epoch [1/50], Step [500/938], D Loss: 0.0363, G Loss: 5.5620, C Loss: -0.0170\n",
      "Epoch [1/50], Step [600/938], D Loss: 0.1798, G Loss: 7.1189, C Loss: -0.0732\n",
      "Epoch [1/50], Step [700/938], D Loss: 0.0139, G Loss: 8.2288, C Loss: -0.0051\n",
      "Epoch [1/50], Step [800/938], D Loss: 0.0255, G Loss: 8.6898, C Loss: -0.0059\n",
      "Epoch [1/50], Step [900/938], D Loss: 0.0410, G Loss: 8.0192, C Loss: -0.0286\n",
      "Epoch [2/50], Step [100/938], D Loss: 0.0113, G Loss: 8.9716, C Loss: -0.0082\n",
      "Epoch [2/50], Step [200/938], D Loss: 0.1325, G Loss: 5.5653, C Loss: -0.0680\n",
      "Epoch [2/50], Step [300/938], D Loss: 0.2911, G Loss: 7.8106, C Loss: -0.0513\n",
      "Epoch [2/50], Step [400/938], D Loss: 0.4984, G Loss: 5.5866, C Loss: -0.1834\n",
      "Epoch [2/50], Step [500/938], D Loss: 0.2586, G Loss: 4.6370, C Loss: -0.1351\n",
      "Epoch [2/50], Step [600/938], D Loss: 0.1138, G Loss: 5.6055, C Loss: -0.0888\n",
      "Epoch [2/50], Step [700/938], D Loss: 0.2302, G Loss: 3.9511, C Loss: -0.1262\n",
      "Epoch [2/50], Step [800/938], D Loss: 0.7927, G Loss: 2.7831, C Loss: -0.5113\n",
      "Epoch [2/50], Step [900/938], D Loss: 1.0758, G Loss: 1.4238, C Loss: -1.2702\n",
      "Epoch [3/50], Step [100/938], D Loss: 2.5601, G Loss: 4.9679, C Loss: -0.8126\n",
      "Epoch [3/50], Step [200/938], D Loss: 1.1936, G Loss: 4.0857, C Loss: -0.6947\n",
      "Epoch [3/50], Step [300/938], D Loss: 0.0327, G Loss: 5.7264, C Loss: -0.0168\n",
      "Epoch [3/50], Step [400/938], D Loss: 0.0299, G Loss: 4.3138, C Loss: -0.0181\n",
      "Epoch [3/50], Step [500/938], D Loss: 0.0638, G Loss: 5.8272, C Loss: -0.0037\n",
      "Epoch [3/50], Step [600/938], D Loss: 0.0134, G Loss: 5.7259, C Loss: -0.0116\n",
      "Epoch [3/50], Step [700/938], D Loss: 0.1529, G Loss: 7.0455, C Loss: -0.0806\n",
      "Epoch [3/50], Step [800/938], D Loss: 0.4617, G Loss: 5.4880, C Loss: -0.1588\n",
      "Epoch [3/50], Step [900/938], D Loss: 0.1625, G Loss: 5.0968, C Loss: -0.0819\n",
      "Epoch [4/50], Step [100/938], D Loss: 0.3302, G Loss: 2.5449, C Loss: -0.2445\n",
      "Epoch [4/50], Step [200/938], D Loss: 0.8266, G Loss: 4.9499, C Loss: -0.2186\n",
      "Epoch [4/50], Step [300/938], D Loss: 0.3425, G Loss: 3.7770, C Loss: -0.2407\n",
      "Epoch [4/50], Step [400/938], D Loss: 1.6252, G Loss: 2.0316, C Loss: -1.1532\n",
      "Epoch [4/50], Step [500/938], D Loss: 1.8489, G Loss: 1.7127, C Loss: -0.6647\n",
      "Epoch [4/50], Step [600/938], D Loss: 0.6276, G Loss: 3.2194, C Loss: -0.2208\n",
      "Epoch [4/50], Step [700/938], D Loss: 0.8863, G Loss: 3.7374, C Loss: -0.1687\n",
      "Epoch [4/50], Step [800/938], D Loss: 0.4257, G Loss: 3.6939, C Loss: -0.2486\n",
      "Epoch [4/50], Step [900/938], D Loss: 1.9854, G Loss: 2.0090, C Loss: -0.7709\n",
      "Epoch [5/50], Step [100/938], D Loss: 1.0365, G Loss: 5.4881, C Loss: -0.2223\n",
      "Epoch [5/50], Step [200/938], D Loss: 0.8645, G Loss: 2.6656, C Loss: -0.5346\n",
      "Epoch [5/50], Step [300/938], D Loss: 2.8225, G Loss: 1.0012, C Loss: -1.4081\n",
      "Epoch [5/50], Step [400/938], D Loss: 2.0137, G Loss: 0.7583, C Loss: -1.0209\n",
      "Epoch [5/50], Step [500/938], D Loss: 1.7436, G Loss: 0.9586, C Loss: -0.7470\n",
      "Epoch [5/50], Step [600/938], D Loss: 0.6741, G Loss: 1.8117, C Loss: -0.3620\n",
      "Epoch [5/50], Step [700/938], D Loss: 1.1908, G Loss: 1.1368, C Loss: -0.6004\n",
      "Epoch [5/50], Step [800/938], D Loss: 0.5120, G Loss: 2.0064, C Loss: -0.2778\n",
      "Epoch [5/50], Step [900/938], D Loss: 0.6233, G Loss: 3.5188, C Loss: -0.3046\n",
      "Epoch [6/50], Step [100/938], D Loss: 1.0475, G Loss: 2.8321, C Loss: -0.5645\n",
      "Epoch [6/50], Step [200/938], D Loss: 2.0303, G Loss: 1.2398, C Loss: -1.1919\n",
      "Epoch [6/50], Step [300/938], D Loss: 0.7073, G Loss: 4.3598, C Loss: -0.3673\n",
      "Epoch [6/50], Step [400/938], D Loss: 2.4473, G Loss: 1.6926, C Loss: -1.6728\n",
      "Epoch [6/50], Step [500/938], D Loss: 1.3209, G Loss: 1.0229, C Loss: -0.6803\n",
      "Epoch [6/50], Step [600/938], D Loss: 0.6255, G Loss: 2.1861, C Loss: -0.2965\n",
      "Epoch [6/50], Step [700/938], D Loss: 0.8073, G Loss: 1.4091, C Loss: -0.3704\n",
      "Epoch [6/50], Step [800/938], D Loss: 0.5182, G Loss: 2.1943, C Loss: -0.1563\n",
      "Epoch [6/50], Step [900/938], D Loss: 0.5427, G Loss: 2.0972, C Loss: -0.2137\n",
      "Epoch [7/50], Step [100/938], D Loss: 0.3814, G Loss: 2.3810, C Loss: -0.1877\n",
      "Epoch [7/50], Step [200/938], D Loss: 0.9252, G Loss: 1.7835, C Loss: -0.3196\n",
      "Epoch [7/50], Step [300/938], D Loss: 0.4978, G Loss: 2.3997, C Loss: -0.1928\n",
      "Epoch [7/50], Step [400/938], D Loss: 0.6232, G Loss: 2.1798, C Loss: -0.3813\n",
      "Epoch [7/50], Step [500/938], D Loss: 0.5594, G Loss: 2.5022, C Loss: -0.2089\n",
      "Epoch [7/50], Step [600/938], D Loss: 0.4696, G Loss: 2.7278, C Loss: -0.1728\n",
      "Epoch [7/50], Step [700/938], D Loss: 0.5255, G Loss: 2.4186, C Loss: -0.2242\n",
      "Epoch [7/50], Step [800/938], D Loss: 0.8315, G Loss: 1.9705, C Loss: -0.3390\n",
      "Epoch [7/50], Step [900/938], D Loss: 0.5151, G Loss: 2.4747, C Loss: -0.2091\n",
      "Epoch [8/50], Step [100/938], D Loss: 0.5148, G Loss: 3.6462, C Loss: -0.1949\n",
      "Epoch [8/50], Step [200/938], D Loss: 0.3412, G Loss: 2.7596, C Loss: -0.2050\n",
      "Epoch [8/50], Step [300/938], D Loss: 0.4880, G Loss: 2.9563, C Loss: -0.1406\n",
      "Epoch [8/50], Step [400/938], D Loss: 0.5755, G Loss: 3.1848, C Loss: -0.2204\n",
      "Epoch [8/50], Step [500/938], D Loss: 0.6483, G Loss: 2.3371, C Loss: -0.3073\n",
      "Epoch [8/50], Step [600/938], D Loss: 0.6050, G Loss: 2.3723, C Loss: -0.2341\n",
      "Epoch [8/50], Step [700/938], D Loss: 0.3507, G Loss: 3.2500, C Loss: -0.1823\n",
      "Epoch [8/50], Step [800/938], D Loss: 1.2348, G Loss: 1.8462, C Loss: -0.3838\n",
      "Epoch [8/50], Step [900/938], D Loss: 0.5225, G Loss: 2.3918, C Loss: -0.2007\n",
      "Epoch [9/50], Step [100/938], D Loss: 0.4333, G Loss: 3.3150, C Loss: -0.1871\n",
      "Epoch [9/50], Step [200/938], D Loss: 1.0527, G Loss: 3.7371, C Loss: -0.1898\n",
      "Epoch [9/50], Step [300/938], D Loss: 0.7674, G Loss: 3.3938, C Loss: -0.2968\n",
      "Epoch [9/50], Step [400/938], D Loss: 1.0166, G Loss: 3.3653, C Loss: -0.4867\n",
      "Epoch [9/50], Step [500/938], D Loss: 0.3237, G Loss: 3.5394, C Loss: -0.1331\n",
      "Epoch [9/50], Step [600/938], D Loss: 0.3630, G Loss: 3.7001, C Loss: -0.0940\n",
      "Epoch [9/50], Step [700/938], D Loss: 0.4489, G Loss: 3.8218, C Loss: -0.1037\n",
      "Epoch [9/50], Step [800/938], D Loss: 0.7651, G Loss: 2.7591, C Loss: -0.2672\n",
      "Epoch [9/50], Step [900/938], D Loss: 0.5408, G Loss: 3.5369, C Loss: -0.0943\n",
      "Epoch [10/50], Step [100/938], D Loss: 0.5622, G Loss: 2.8237, C Loss: -0.2617\n",
      "Epoch [10/50], Step [200/938], D Loss: 0.6066, G Loss: 4.0862, C Loss: -0.1402\n",
      "Epoch [10/50], Step [300/938], D Loss: 0.5574, G Loss: 3.6736, C Loss: -0.3102\n",
      "Epoch [10/50], Step [400/938], D Loss: 0.2405, G Loss: 4.3238, C Loss: -0.3005\n",
      "Epoch [10/50], Step [500/938], D Loss: 0.4002, G Loss: 4.9303, C Loss: -0.1485\n",
      "Epoch [10/50], Step [600/938], D Loss: 0.2939, G Loss: 4.4188, C Loss: -0.1604\n",
      "Epoch [10/50], Step [700/938], D Loss: 0.4952, G Loss: 4.3261, C Loss: -0.2983\n",
      "Epoch [10/50], Step [800/938], D Loss: 0.5498, G Loss: 2.9402, C Loss: -0.2017\n",
      "Epoch [10/50], Step [900/938], D Loss: 0.5010, G Loss: 3.2478, C Loss: -0.2549\n",
      "Epoch [11/50], Step [100/938], D Loss: 1.4752, G Loss: 2.9361, C Loss: -0.7149\n",
      "Epoch [11/50], Step [200/938], D Loss: 0.4723, G Loss: 4.8819, C Loss: -0.3644\n",
      "Epoch [11/50], Step [300/938], D Loss: 0.8904, G Loss: 3.3947, C Loss: -0.3682\n",
      "Epoch [11/50], Step [400/938], D Loss: 0.3478, G Loss: 3.0006, C Loss: -0.1239\n",
      "Epoch [11/50], Step [500/938], D Loss: 0.4717, G Loss: 4.2385, C Loss: -0.0585\n",
      "Epoch [11/50], Step [600/938], D Loss: 1.1168, G Loss: 1.8801, C Loss: -0.5585\n",
      "Epoch [11/50], Step [700/938], D Loss: 0.6664, G Loss: 3.0541, C Loss: -0.2462\n",
      "Epoch [11/50], Step [800/938], D Loss: 0.9119, G Loss: 4.0205, C Loss: -0.2202\n",
      "Epoch [11/50], Step [900/938], D Loss: 0.8387, G Loss: 3.0476, C Loss: -0.2933\n",
      "Epoch [12/50], Step [100/938], D Loss: 0.4421, G Loss: 2.0506, C Loss: -0.2982\n",
      "Epoch [12/50], Step [200/938], D Loss: 0.3912, G Loss: 2.1022, C Loss: -0.3993\n",
      "Epoch [12/50], Step [300/938], D Loss: 0.7249, G Loss: 2.9883, C Loss: -0.2536\n",
      "Epoch [12/50], Step [400/938], D Loss: 0.4410, G Loss: 2.9148, C Loss: -0.2985\n",
      "Epoch [12/50], Step [500/938], D Loss: 0.5623, G Loss: 3.3438, C Loss: -0.1589\n",
      "Epoch [12/50], Step [600/938], D Loss: 0.6880, G Loss: 2.5008, C Loss: -0.3118\n",
      "Epoch [12/50], Step [700/938], D Loss: 0.3788, G Loss: 3.5791, C Loss: -0.0871\n",
      "Epoch [12/50], Step [800/938], D Loss: 0.4695, G Loss: 2.7172, C Loss: -0.2049\n",
      "Epoch [12/50], Step [900/938], D Loss: 0.3574, G Loss: 3.4850, C Loss: -0.1621\n",
      "Epoch [13/50], Step [100/938], D Loss: 0.7459, G Loss: 3.9045, C Loss: -0.1895\n",
      "Epoch [13/50], Step [200/938], D Loss: 0.5808, G Loss: 3.4345, C Loss: -0.1967\n",
      "Epoch [13/50], Step [300/938], D Loss: 0.3429, G Loss: 3.5379, C Loss: -0.1504\n",
      "Epoch [13/50], Step [400/938], D Loss: 0.5586, G Loss: 3.9027, C Loss: -0.0683\n",
      "Epoch [13/50], Step [500/938], D Loss: 0.6654, G Loss: 2.7280, C Loss: -0.2501\n",
      "Epoch [13/50], Step [600/938], D Loss: 0.5839, G Loss: 4.0807, C Loss: -0.1558\n",
      "Epoch [13/50], Step [700/938], D Loss: 0.3907, G Loss: 3.1553, C Loss: -0.1867\n",
      "Epoch [13/50], Step [800/938], D Loss: 0.4158, G Loss: 3.1227, C Loss: -0.2116\n",
      "Epoch [13/50], Step [900/938], D Loss: 0.4435, G Loss: 3.8771, C Loss: -0.1221\n",
      "Epoch [14/50], Step [100/938], D Loss: 0.3795, G Loss: 3.0420, C Loss: -0.2365\n",
      "Epoch [14/50], Step [200/938], D Loss: 0.5118, G Loss: 3.1492, C Loss: -0.2306\n",
      "Epoch [14/50], Step [300/938], D Loss: 0.5407, G Loss: 2.9218, C Loss: -0.1438\n",
      "Epoch [14/50], Step [400/938], D Loss: 0.7245, G Loss: 2.3981, C Loss: -0.3556\n",
      "Epoch [14/50], Step [500/938], D Loss: 0.3544, G Loss: 3.6716, C Loss: -0.1704\n",
      "Epoch [14/50], Step [600/938], D Loss: 1.0852, G Loss: 6.5314, C Loss: -0.3662\n",
      "Epoch [14/50], Step [700/938], D Loss: 0.2324, G Loss: 4.7987, C Loss: -0.0310\n",
      "Epoch [14/50], Step [800/938], D Loss: 0.5805, G Loss: 4.7649, C Loss: -0.1060\n",
      "Epoch [14/50], Step [900/938], D Loss: 0.4115, G Loss: 4.9219, C Loss: -0.2686\n",
      "Epoch [15/50], Step [100/938], D Loss: 0.5294, G Loss: 4.5027, C Loss: -0.0832\n",
      "Epoch [15/50], Step [200/938], D Loss: 0.1503, G Loss: 7.2608, C Loss: -0.0075\n",
      "Epoch [15/50], Step [300/938], D Loss: 0.3435, G Loss: 5.4841, C Loss: -0.1051\n",
      "Epoch [15/50], Step [400/938], D Loss: 0.2518, G Loss: 5.0697, C Loss: -0.1772\n",
      "Epoch [15/50], Step [500/938], D Loss: 0.2635, G Loss: 4.8595, C Loss: -0.1494\n",
      "Epoch [15/50], Step [600/938], D Loss: 0.5217, G Loss: 5.2906, C Loss: -0.0525\n",
      "Epoch [15/50], Step [700/938], D Loss: 0.2780, G Loss: 3.3917, C Loss: -0.1715\n",
      "Epoch [15/50], Step [800/938], D Loss: 0.2166, G Loss: 3.6244, C Loss: -0.1306\n",
      "Epoch [15/50], Step [900/938], D Loss: 0.6454, G Loss: 3.2160, C Loss: -0.2019\n",
      "Epoch [16/50], Step [100/938], D Loss: 1.0655, G Loss: 3.4023, C Loss: -0.3453\n",
      "Epoch [16/50], Step [200/938], D Loss: 0.3986, G Loss: 4.9020, C Loss: -0.2129\n",
      "Epoch [16/50], Step [300/938], D Loss: 0.5380, G Loss: 4.2194, C Loss: -0.1108\n",
      "Epoch [16/50], Step [400/938], D Loss: 0.3268, G Loss: 3.7531, C Loss: -0.1320\n",
      "Epoch [16/50], Step [500/938], D Loss: 0.4875, G Loss: 3.7644, C Loss: -0.2409\n",
      "Epoch [16/50], Step [600/938], D Loss: 0.2569, G Loss: 3.8134, C Loss: -0.1022\n",
      "Epoch [16/50], Step [700/938], D Loss: 0.3897, G Loss: 3.7398, C Loss: -0.1882\n",
      "Epoch [16/50], Step [800/938], D Loss: 0.2732, G Loss: 3.3495, C Loss: -0.0987\n",
      "Epoch [16/50], Step [900/938], D Loss: 0.5607, G Loss: 4.3176, C Loss: -0.1904\n",
      "Epoch [17/50], Step [100/938], D Loss: 0.3017, G Loss: 4.9647, C Loss: -0.1695\n",
      "Epoch [17/50], Step [200/938], D Loss: 0.3079, G Loss: 5.7881, C Loss: -0.0594\n",
      "Epoch [17/50], Step [300/938], D Loss: 0.0988, G Loss: 4.1171, C Loss: -0.0517\n",
      "Epoch [17/50], Step [400/938], D Loss: 0.6104, G Loss: 3.7643, C Loss: -0.2549\n",
      "Epoch [17/50], Step [500/938], D Loss: 0.6507, G Loss: 4.7908, C Loss: -0.1822\n",
      "Epoch [17/50], Step [600/938], D Loss: 0.5513, G Loss: 5.3118, C Loss: -0.0345\n",
      "Epoch [17/50], Step [700/938], D Loss: 0.4669, G Loss: 2.9124, C Loss: -0.1881\n",
      "Epoch [17/50], Step [800/938], D Loss: 0.4960, G Loss: 3.7071, C Loss: -0.1017\n",
      "Epoch [17/50], Step [900/938], D Loss: 0.2819, G Loss: 3.4532, C Loss: -0.1084\n",
      "Epoch [18/50], Step [100/938], D Loss: 0.3442, G Loss: 3.6454, C Loss: -0.2866\n",
      "Epoch [18/50], Step [200/938], D Loss: 0.3857, G Loss: 5.1316, C Loss: -0.2045\n",
      "Epoch [18/50], Step [300/938], D Loss: 0.1477, G Loss: 5.4143, C Loss: -0.0273\n",
      "Epoch [18/50], Step [400/938], D Loss: 0.2260, G Loss: 7.8299, C Loss: -0.0123\n",
      "Epoch [18/50], Step [500/938], D Loss: 0.6891, G Loss: 2.1779, C Loss: -0.4814\n",
      "Epoch [18/50], Step [600/938], D Loss: 0.2615, G Loss: 3.0316, C Loss: -0.1277\n",
      "Epoch [18/50], Step [700/938], D Loss: 0.4755, G Loss: 2.4909, C Loss: -0.2750\n",
      "Epoch [18/50], Step [800/938], D Loss: 0.4324, G Loss: 3.5550, C Loss: -0.1594\n",
      "Epoch [18/50], Step [900/938], D Loss: 0.4689, G Loss: 4.3219, C Loss: -0.0657\n",
      "Epoch [19/50], Step [100/938], D Loss: 0.2863, G Loss: 2.8470, C Loss: -0.1245\n",
      "Epoch [19/50], Step [200/938], D Loss: 0.3609, G Loss: 3.2197, C Loss: -0.1740\n",
      "Epoch [19/50], Step [300/938], D Loss: 0.5729, G Loss: 3.7187, C Loss: -0.1829\n",
      "Epoch [19/50], Step [400/938], D Loss: 0.3636, G Loss: 6.4611, C Loss: -0.0620\n",
      "Epoch [19/50], Step [500/938], D Loss: 0.3479, G Loss: 6.8938, C Loss: -0.0481\n",
      "Epoch [19/50], Step [600/938], D Loss: 0.3624, G Loss: 6.0042, C Loss: -0.1446\n",
      "Epoch [19/50], Step [700/938], D Loss: 0.4874, G Loss: 4.9776, C Loss: -0.0859\n",
      "Epoch [19/50], Step [800/938], D Loss: 0.3426, G Loss: 5.2056, C Loss: -0.0825\n",
      "Epoch [19/50], Step [900/938], D Loss: 0.6498, G Loss: 2.8267, C Loss: -0.2798\n",
      "Epoch [20/50], Step [100/938], D Loss: 0.3451, G Loss: 4.8804, C Loss: -0.1922\n",
      "Epoch [20/50], Step [200/938], D Loss: 0.5377, G Loss: 4.0235, C Loss: -0.1752\n",
      "Epoch [20/50], Step [300/938], D Loss: 1.1226, G Loss: 4.2977, C Loss: -0.2352\n",
      "Epoch [20/50], Step [400/938], D Loss: 0.4538, G Loss: 4.3945, C Loss: -0.0838\n",
      "Epoch [20/50], Step [500/938], D Loss: 0.4854, G Loss: 2.8560, C Loss: -0.2719\n",
      "Epoch [20/50], Step [600/938], D Loss: 0.5423, G Loss: 4.2354, C Loss: -0.0456\n",
      "Epoch [20/50], Step [700/938], D Loss: 0.3510, G Loss: 3.9824, C Loss: -0.0684\n",
      "Epoch [20/50], Step [800/938], D Loss: 0.3538, G Loss: 3.4330, C Loss: -0.0753\n",
      "Epoch [20/50], Step [900/938], D Loss: 0.5995, G Loss: 3.9377, C Loss: -0.1710\n",
      "Epoch [21/50], Step [100/938], D Loss: 0.4730, G Loss: 3.3529, C Loss: -0.1111\n",
      "Epoch [21/50], Step [200/938], D Loss: 0.6718, G Loss: 2.6609, C Loss: -0.2137\n",
      "Epoch [21/50], Step [300/938], D Loss: 0.4929, G Loss: 2.7824, C Loss: -0.1777\n",
      "Epoch [21/50], Step [400/938], D Loss: 0.6842, G Loss: 2.4727, C Loss: -0.2562\n",
      "Epoch [21/50], Step [500/938], D Loss: 0.4739, G Loss: 3.9163, C Loss: -0.1019\n",
      "Epoch [21/50], Step [600/938], D Loss: 0.7182, G Loss: 2.8563, C Loss: -0.1930\n",
      "Epoch [21/50], Step [700/938], D Loss: 1.1306, G Loss: 3.1966, C Loss: -0.2999\n",
      "Epoch [21/50], Step [800/938], D Loss: 0.5923, G Loss: 3.0521, C Loss: -0.2228\n",
      "Epoch [21/50], Step [900/938], D Loss: 0.3138, G Loss: 3.0576, C Loss: -0.1430\n",
      "Epoch [22/50], Step [100/938], D Loss: 0.2795, G Loss: 3.5684, C Loss: -0.0790\n",
      "Epoch [22/50], Step [200/938], D Loss: 0.4725, G Loss: 3.0843, C Loss: -0.2090\n",
      "Epoch [22/50], Step [300/938], D Loss: 1.0143, G Loss: 3.5140, C Loss: -0.2905\n",
      "Epoch [22/50], Step [400/938], D Loss: 1.0258, G Loss: 2.7965, C Loss: -0.4683\n",
      "Epoch [22/50], Step [500/938], D Loss: 0.5183, G Loss: 2.4381, C Loss: -0.2693\n",
      "Epoch [22/50], Step [600/938], D Loss: 0.4697, G Loss: 3.4172, C Loss: -0.2192\n",
      "Epoch [22/50], Step [700/938], D Loss: 0.6291, G Loss: 3.1812, C Loss: -0.2124\n",
      "Epoch [22/50], Step [800/938], D Loss: 1.0245, G Loss: 2.6315, C Loss: -0.2605\n",
      "Epoch [22/50], Step [900/938], D Loss: 0.4691, G Loss: 2.3613, C Loss: -0.2070\n",
      "Epoch [23/50], Step [100/938], D Loss: 0.5336, G Loss: 2.4217, C Loss: -0.2173\n",
      "Epoch [23/50], Step [200/938], D Loss: 0.3015, G Loss: 3.6126, C Loss: -0.0703\n",
      "Epoch [23/50], Step [300/938], D Loss: 0.3302, G Loss: 3.1901, C Loss: -0.2134\n",
      "Epoch [23/50], Step [400/938], D Loss: 0.4034, G Loss: 3.7842, C Loss: -0.0646\n",
      "Epoch [23/50], Step [500/938], D Loss: 0.7952, G Loss: 3.1551, C Loss: -0.2212\n",
      "Epoch [23/50], Step [600/938], D Loss: 0.5595, G Loss: 3.1615, C Loss: -0.1426\n",
      "Epoch [23/50], Step [700/938], D Loss: 0.6118, G Loss: 1.9191, C Loss: -0.2888\n",
      "Epoch [23/50], Step [800/938], D Loss: 0.4049, G Loss: 2.6074, C Loss: -0.2181\n",
      "Epoch [23/50], Step [900/938], D Loss: 0.6729, G Loss: 2.6790, C Loss: -0.2347\n",
      "Epoch [24/50], Step [100/938], D Loss: 0.8242, G Loss: 3.8109, C Loss: -0.1652\n",
      "Epoch [24/50], Step [200/938], D Loss: 0.4000, G Loss: 3.8221, C Loss: -0.1361\n",
      "Epoch [24/50], Step [300/938], D Loss: 0.4363, G Loss: 3.8578, C Loss: -0.1631\n",
      "Epoch [24/50], Step [400/938], D Loss: 1.0158, G Loss: 3.2231, C Loss: -0.3278\n",
      "Epoch [24/50], Step [500/938], D Loss: 0.7673, G Loss: 1.8968, C Loss: -0.4465\n",
      "Epoch [24/50], Step [600/938], D Loss: 0.6440, G Loss: 3.6560, C Loss: -0.0970\n",
      "Epoch [24/50], Step [700/938], D Loss: 0.4680, G Loss: 4.5720, C Loss: -0.1436\n",
      "Epoch [24/50], Step [800/938], D Loss: 0.6087, G Loss: 3.9328, C Loss: -0.2113\n",
      "Epoch [24/50], Step [900/938], D Loss: 1.1316, G Loss: 2.2820, C Loss: -0.5393\n",
      "Epoch [25/50], Step [100/938], D Loss: 0.7794, G Loss: 3.3041, C Loss: -0.4638\n",
      "Epoch [25/50], Step [200/938], D Loss: 0.3094, G Loss: 4.6985, C Loss: -0.1271\n",
      "Epoch [25/50], Step [300/938], D Loss: 0.6657, G Loss: 3.3109, C Loss: -0.2663\n",
      "Epoch [25/50], Step [400/938], D Loss: 0.5477, G Loss: 4.1923, C Loss: -0.0892\n",
      "Epoch [25/50], Step [500/938], D Loss: 0.4676, G Loss: 3.1912, C Loss: -0.2821\n",
      "Epoch [25/50], Step [600/938], D Loss: 0.4202, G Loss: 3.2825, C Loss: -0.2040\n",
      "Epoch [25/50], Step [700/938], D Loss: 0.3758, G Loss: 3.5095, C Loss: -0.1716\n",
      "Epoch [25/50], Step [800/938], D Loss: 0.3872, G Loss: 3.4380, C Loss: -0.1819\n",
      "Epoch [25/50], Step [900/938], D Loss: 0.2820, G Loss: 4.3433, C Loss: -0.0655\n",
      "Epoch [26/50], Step [100/938], D Loss: 0.6346, G Loss: 5.3680, C Loss: -0.0682\n",
      "Epoch [26/50], Step [200/938], D Loss: 0.4061, G Loss: 2.1692, C Loss: -0.3001\n",
      "Epoch [26/50], Step [300/938], D Loss: 0.3238, G Loss: 2.8256, C Loss: -0.2708\n",
      "Epoch [26/50], Step [400/938], D Loss: 0.2967, G Loss: 4.6839, C Loss: -0.0417\n",
      "Epoch [26/50], Step [500/938], D Loss: 0.4558, G Loss: 3.2401, C Loss: -0.2112\n",
      "Epoch [26/50], Step [600/938], D Loss: 0.6808, G Loss: 3.5493, C Loss: -0.1768\n",
      "Epoch [26/50], Step [700/938], D Loss: 0.5246, G Loss: 2.8996, C Loss: -0.1739\n",
      "Epoch [26/50], Step [800/938], D Loss: 0.3634, G Loss: 2.7567, C Loss: -0.1716\n",
      "Epoch [26/50], Step [900/938], D Loss: 0.3636, G Loss: 3.0481, C Loss: -0.1092\n",
      "Epoch [27/50], Step [100/938], D Loss: 0.6654, G Loss: 2.7859, C Loss: -0.2318\n",
      "Epoch [27/50], Step [200/938], D Loss: 0.5857, G Loss: 2.2516, C Loss: -0.3667\n",
      "Epoch [27/50], Step [300/938], D Loss: 0.4691, G Loss: 3.4267, C Loss: -0.1188\n",
      "Epoch [27/50], Step [400/938], D Loss: 0.6460, G Loss: 2.3437, C Loss: -0.2996\n",
      "Epoch [27/50], Step [500/938], D Loss: 0.4649, G Loss: 3.8948, C Loss: -0.1023\n",
      "Epoch [27/50], Step [600/938], D Loss: 0.4161, G Loss: 3.2041, C Loss: -0.2221\n",
      "Epoch [27/50], Step [700/938], D Loss: 0.4889, G Loss: 3.9435, C Loss: -0.1084\n",
      "Epoch [27/50], Step [800/938], D Loss: 0.3312, G Loss: 3.2385, C Loss: -0.1584\n",
      "Epoch [27/50], Step [900/938], D Loss: 0.5833, G Loss: 2.7974, C Loss: -0.2161\n",
      "Epoch [28/50], Step [100/938], D Loss: 0.6190, G Loss: 2.7603, C Loss: -0.2750\n",
      "Epoch [28/50], Step [200/938], D Loss: 0.5491, G Loss: 3.1141, C Loss: -0.2272\n",
      "Epoch [28/50], Step [300/938], D Loss: 0.4623, G Loss: 3.0568, C Loss: -0.2312\n",
      "Epoch [28/50], Step [400/938], D Loss: 0.4683, G Loss: 2.9011, C Loss: -0.2093\n",
      "Epoch [28/50], Step [500/938], D Loss: 1.2238, G Loss: 3.5601, C Loss: -0.1019\n",
      "Epoch [28/50], Step [600/938], D Loss: 0.7616, G Loss: 1.5203, C Loss: -0.6592\n",
      "Epoch [28/50], Step [700/938], D Loss: 0.5511, G Loss: 2.6008, C Loss: -0.2432\n",
      "Epoch [28/50], Step [800/938], D Loss: 0.8058, G Loss: 2.8951, C Loss: -0.2670\n",
      "Epoch [28/50], Step [900/938], D Loss: 0.8870, G Loss: 2.0123, C Loss: -0.4815\n",
      "Epoch [29/50], Step [100/938], D Loss: 0.4239, G Loss: 3.6713, C Loss: -0.1100\n",
      "Epoch [29/50], Step [200/938], D Loss: 0.5335, G Loss: 2.7131, C Loss: -0.2441\n",
      "Epoch [29/50], Step [300/938], D Loss: 0.4615, G Loss: 3.3554, C Loss: -0.2616\n",
      "Epoch [29/50], Step [400/938], D Loss: 0.4004, G Loss: 2.8432, C Loss: -0.2302\n",
      "Epoch [29/50], Step [500/938], D Loss: 0.6247, G Loss: 2.0191, C Loss: -0.5617\n",
      "Epoch [29/50], Step [600/938], D Loss: 0.5039, G Loss: 3.5381, C Loss: -0.1004\n",
      "Epoch [29/50], Step [700/938], D Loss: 0.4468, G Loss: 2.9077, C Loss: -0.2126\n",
      "Epoch [29/50], Step [800/938], D Loss: 0.4212, G Loss: 3.3918, C Loss: -0.1257\n",
      "Epoch [29/50], Step [900/938], D Loss: 0.4030, G Loss: 2.5241, C Loss: -0.2600\n",
      "Epoch [30/50], Step [100/938], D Loss: 0.2493, G Loss: 3.7974, C Loss: -0.0688\n",
      "Epoch [30/50], Step [200/938], D Loss: 0.5000, G Loss: 2.7840, C Loss: -0.1708\n",
      "Epoch [30/50], Step [300/938], D Loss: 0.5315, G Loss: 3.4110, C Loss: -0.2540\n",
      "Epoch [30/50], Step [400/938], D Loss: 0.7126, G Loss: 2.3192, C Loss: -0.2981\n",
      "Epoch [30/50], Step [500/938], D Loss: 0.7823, G Loss: 1.8557, C Loss: -0.4065\n",
      "Epoch [30/50], Step [600/938], D Loss: 0.5469, G Loss: 2.6120, C Loss: -0.2487\n",
      "Epoch [30/50], Step [700/938], D Loss: 0.4248, G Loss: 2.6883, C Loss: -0.2183\n",
      "Epoch [30/50], Step [800/938], D Loss: 0.4241, G Loss: 2.4827, C Loss: -0.2494\n",
      "Epoch [30/50], Step [900/938], D Loss: 0.2839, G Loss: 3.2270, C Loss: -0.1166\n",
      "Epoch [31/50], Step [100/938], D Loss: 0.6576, G Loss: 2.9630, C Loss: -0.1285\n",
      "Epoch [31/50], Step [200/938], D Loss: 0.6323, G Loss: 2.2720, C Loss: -0.2969\n",
      "Epoch [31/50], Step [300/938], D Loss: 0.6161, G Loss: 2.4949, C Loss: -0.2387\n",
      "Epoch [31/50], Step [400/938], D Loss: 0.6936, G Loss: 2.2847, C Loss: -0.3314\n",
      "Epoch [31/50], Step [500/938], D Loss: 0.3727, G Loss: 2.1076, C Loss: -0.3550\n",
      "Epoch [31/50], Step [600/938], D Loss: 0.8518, G Loss: 1.7892, C Loss: -0.4715\n",
      "Epoch [31/50], Step [700/938], D Loss: 0.4257, G Loss: 2.5647, C Loss: -0.2552\n",
      "Epoch [31/50], Step [800/938], D Loss: 0.5894, G Loss: 2.6343, C Loss: -0.1545\n",
      "Epoch [31/50], Step [900/938], D Loss: 0.4507, G Loss: 3.1045, C Loss: -0.1710\n",
      "Epoch [32/50], Step [100/938], D Loss: 0.5460, G Loss: 2.0522, C Loss: -0.3545\n",
      "Epoch [32/50], Step [200/938], D Loss: 0.4575, G Loss: 1.9758, C Loss: -0.3121\n",
      "Epoch [32/50], Step [300/938], D Loss: 0.8567, G Loss: 1.7592, C Loss: -0.3883\n",
      "Epoch [32/50], Step [400/938], D Loss: 0.4233, G Loss: 2.7708, C Loss: -0.1894\n",
      "Epoch [32/50], Step [500/938], D Loss: 0.6147, G Loss: 2.6468, C Loss: -0.2450\n",
      "Epoch [32/50], Step [600/938], D Loss: 0.5748, G Loss: 2.1441, C Loss: -0.2662\n",
      "Epoch [32/50], Step [700/938], D Loss: 0.4660, G Loss: 2.7819, C Loss: -0.2904\n",
      "Epoch [32/50], Step [800/938], D Loss: 0.6072, G Loss: 2.6433, C Loss: -0.2384\n",
      "Epoch [32/50], Step [900/938], D Loss: 0.7967, G Loss: 1.8171, C Loss: -0.3868\n",
      "Epoch [33/50], Step [100/938], D Loss: 0.4838, G Loss: 2.7283, C Loss: -0.1905\n",
      "Epoch [33/50], Step [200/938], D Loss: 0.6733, G Loss: 3.1311, C Loss: -0.1408\n",
      "Epoch [33/50], Step [300/938], D Loss: 0.4825, G Loss: 2.5769, C Loss: -0.2194\n",
      "Epoch [33/50], Step [400/938], D Loss: 0.5259, G Loss: 3.5430, C Loss: -0.1027\n",
      "Epoch [33/50], Step [500/938], D Loss: 0.6297, G Loss: 2.5476, C Loss: -0.2051\n",
      "Epoch [33/50], Step [600/938], D Loss: 0.4693, G Loss: 3.1906, C Loss: -0.1666\n",
      "Epoch [33/50], Step [700/938], D Loss: 0.8732, G Loss: 3.1813, C Loss: -0.2027\n",
      "Epoch [33/50], Step [800/938], D Loss: 0.7395, G Loss: 2.3148, C Loss: -0.2786\n",
      "Epoch [33/50], Step [900/938], D Loss: 0.5940, G Loss: 2.7045, C Loss: -0.2936\n",
      "Epoch [34/50], Step [100/938], D Loss: 0.5293, G Loss: 2.2753, C Loss: -0.3035\n",
      "Epoch [34/50], Step [200/938], D Loss: 0.6954, G Loss: 1.8276, C Loss: -0.4632\n",
      "Epoch [34/50], Step [300/938], D Loss: 0.6191, G Loss: 1.9322, C Loss: -0.4069\n",
      "Epoch [34/50], Step [400/938], D Loss: 0.8829, G Loss: 2.0569, C Loss: -0.3094\n",
      "Epoch [34/50], Step [500/938], D Loss: 0.7069, G Loss: 3.3029, C Loss: -0.1159\n",
      "Epoch [34/50], Step [600/938], D Loss: 0.5769, G Loss: 3.1384, C Loss: -0.1281\n",
      "Epoch [34/50], Step [700/938], D Loss: 0.5892, G Loss: 2.4422, C Loss: -0.2740\n",
      "Epoch [34/50], Step [800/938], D Loss: 0.4848, G Loss: 2.9512, C Loss: -0.1955\n",
      "Epoch [34/50], Step [900/938], D Loss: 0.4459, G Loss: 3.7902, C Loss: -0.0772\n",
      "Epoch [35/50], Step [100/938], D Loss: 0.6109, G Loss: 2.9498, C Loss: -0.2068\n",
      "Epoch [35/50], Step [200/938], D Loss: 0.4588, G Loss: 2.5182, C Loss: -0.1877\n",
      "Epoch [35/50], Step [300/938], D Loss: 0.5398, G Loss: 2.4383, C Loss: -0.3067\n",
      "Epoch [35/50], Step [400/938], D Loss: 0.5649, G Loss: 2.4445, C Loss: -0.2578\n",
      "Epoch [35/50], Step [500/938], D Loss: 0.7069, G Loss: 2.5639, C Loss: -0.2349\n",
      "Epoch [35/50], Step [600/938], D Loss: 0.4165, G Loss: 2.2724, C Loss: -0.2159\n",
      "Epoch [35/50], Step [700/938], D Loss: 0.5590, G Loss: 2.5853, C Loss: -0.2868\n",
      "Epoch [35/50], Step [800/938], D Loss: 0.6119, G Loss: 2.3132, C Loss: -0.2836\n",
      "Epoch [35/50], Step [900/938], D Loss: 0.5329, G Loss: 2.4018, C Loss: -0.2058\n",
      "Epoch [36/50], Step [100/938], D Loss: 0.7409, G Loss: 2.1014, C Loss: -0.3580\n",
      "Epoch [36/50], Step [200/938], D Loss: 0.4139, G Loss: 2.5083, C Loss: -0.2079\n",
      "Epoch [36/50], Step [300/938], D Loss: 0.5775, G Loss: 2.9978, C Loss: -0.1741\n",
      "Epoch [36/50], Step [400/938], D Loss: 0.5267, G Loss: 2.7366, C Loss: -0.1399\n",
      "Epoch [36/50], Step [500/938], D Loss: 0.5182, G Loss: 1.5604, C Loss: -0.4496\n",
      "Epoch [36/50], Step [600/938], D Loss: 0.6403, G Loss: 1.9967, C Loss: -0.3444\n",
      "Epoch [36/50], Step [700/938], D Loss: 0.5494, G Loss: 2.0554, C Loss: -0.4035\n",
      "Epoch [36/50], Step [800/938], D Loss: 0.4906, G Loss: 2.8666, C Loss: -0.1887\n",
      "Epoch [36/50], Step [900/938], D Loss: 0.7078, G Loss: 1.9994, C Loss: -0.3399\n",
      "Epoch [37/50], Step [100/938], D Loss: 0.7117, G Loss: 2.2803, C Loss: -0.2840\n",
      "Epoch [37/50], Step [200/938], D Loss: 0.7413, G Loss: 2.0471, C Loss: -0.3559\n",
      "Epoch [37/50], Step [300/938], D Loss: 0.7453, G Loss: 2.3689, C Loss: -0.2602\n",
      "Epoch [37/50], Step [400/938], D Loss: 0.9703, G Loss: 1.9939, C Loss: -0.3211\n",
      "Epoch [37/50], Step [500/938], D Loss: 0.6414, G Loss: 2.1760, C Loss: -0.3341\n",
      "Epoch [37/50], Step [600/938], D Loss: 0.5929, G Loss: 2.3974, C Loss: -0.2250\n",
      "Epoch [37/50], Step [700/938], D Loss: 0.6182, G Loss: 2.4533, C Loss: -0.2233\n",
      "Epoch [37/50], Step [800/938], D Loss: 0.8665, G Loss: 2.0678, C Loss: -0.3073\n",
      "Epoch [37/50], Step [900/938], D Loss: 0.7188, G Loss: 1.9105, C Loss: -0.3632\n",
      "Epoch [38/50], Step [100/938], D Loss: 0.9407, G Loss: 2.6015, C Loss: -0.2507\n",
      "Epoch [38/50], Step [200/938], D Loss: 0.6501, G Loss: 2.3461, C Loss: -0.2207\n",
      "Epoch [38/50], Step [300/938], D Loss: 1.1807, G Loss: 1.4897, C Loss: -0.5570\n",
      "Epoch [38/50], Step [400/938], D Loss: 0.7012, G Loss: 1.8786, C Loss: -0.3628\n",
      "Epoch [38/50], Step [500/938], D Loss: 0.9840, G Loss: 2.0324, C Loss: -0.4094\n",
      "Epoch [38/50], Step [600/938], D Loss: 1.0074, G Loss: 1.7342, C Loss: -0.4412\n",
      "Epoch [38/50], Step [700/938], D Loss: 0.8943, G Loss: 1.6846, C Loss: -0.4484\n",
      "Epoch [38/50], Step [800/938], D Loss: 0.7592, G Loss: 2.1484, C Loss: -0.2956\n",
      "Epoch [38/50], Step [900/938], D Loss: 0.8621, G Loss: 1.4228, C Loss: -0.4952\n",
      "Epoch [39/50], Step [100/938], D Loss: 0.8964, G Loss: 2.0151, C Loss: -0.3437\n",
      "Epoch [39/50], Step [200/938], D Loss: 0.8311, G Loss: 2.1757, C Loss: -0.3053\n",
      "Epoch [39/50], Step [300/938], D Loss: 0.6556, G Loss: 1.7958, C Loss: -0.3577\n",
      "Epoch [39/50], Step [400/938], D Loss: 0.8298, G Loss: 1.7101, C Loss: -0.4173\n",
      "Epoch [39/50], Step [500/938], D Loss: 0.7982, G Loss: 1.9977, C Loss: -0.3638\n",
      "Epoch [39/50], Step [600/938], D Loss: 0.8240, G Loss: 2.6286, C Loss: -0.2363\n",
      "Epoch [39/50], Step [700/938], D Loss: 0.9609, G Loss: 1.8431, C Loss: -0.4238\n",
      "Epoch [39/50], Step [800/938], D Loss: 0.8481, G Loss: 1.6523, C Loss: -0.5889\n",
      "Epoch [39/50], Step [900/938], D Loss: 0.7284, G Loss: 1.8951, C Loss: -0.3612\n",
      "Epoch [40/50], Step [100/938], D Loss: 0.8236, G Loss: 1.5855, C Loss: -0.5405\n",
      "Epoch [40/50], Step [200/938], D Loss: 0.7678, G Loss: 2.1494, C Loss: -0.3094\n",
      "Epoch [40/50], Step [300/938], D Loss: 0.7178, G Loss: 2.2796, C Loss: -0.3253\n",
      "Epoch [40/50], Step [400/938], D Loss: 0.8795, G Loss: 2.0187, C Loss: -0.3032\n",
      "Epoch [40/50], Step [500/938], D Loss: 0.7489, G Loss: 2.1648, C Loss: -0.2906\n",
      "Epoch [40/50], Step [600/938], D Loss: 0.6199, G Loss: 1.9194, C Loss: -0.4367\n",
      "Epoch [40/50], Step [700/938], D Loss: 0.7506, G Loss: 1.8472, C Loss: -0.3924\n",
      "Epoch [40/50], Step [800/938], D Loss: 0.8014, G Loss: 1.7065, C Loss: -0.4135\n",
      "Epoch [40/50], Step [900/938], D Loss: 0.7346, G Loss: 2.0927, C Loss: -0.3121\n",
      "Epoch [41/50], Step [100/938], D Loss: 0.6479, G Loss: 1.8998, C Loss: -0.3370\n",
      "Epoch [41/50], Step [200/938], D Loss: 0.6664, G Loss: 2.2385, C Loss: -0.2923\n",
      "Epoch [41/50], Step [300/938], D Loss: 0.8665, G Loss: 1.8074, C Loss: -0.4936\n",
      "Epoch [41/50], Step [400/938], D Loss: 0.9506, G Loss: 3.4103, C Loss: -0.1958\n",
      "Epoch [41/50], Step [500/938], D Loss: 0.7525, G Loss: 1.9664, C Loss: -0.3614\n",
      "Epoch [41/50], Step [600/938], D Loss: 0.7664, G Loss: 1.9875, C Loss: -0.3242\n",
      "Epoch [41/50], Step [700/938], D Loss: 0.8493, G Loss: 1.8598, C Loss: -0.4302\n",
      "Epoch [41/50], Step [800/938], D Loss: 0.6911, G Loss: 2.4543, C Loss: -0.2560\n",
      "Epoch [41/50], Step [900/938], D Loss: 0.8653, G Loss: 1.7973, C Loss: -0.4180\n",
      "Epoch [42/50], Step [100/938], D Loss: 0.8663, G Loss: 1.9626, C Loss: -0.3573\n",
      "Epoch [42/50], Step [200/938], D Loss: 0.6802, G Loss: 2.6113, C Loss: -0.2000\n",
      "Epoch [42/50], Step [300/938], D Loss: 0.8056, G Loss: 1.6630, C Loss: -0.4206\n",
      "Epoch [42/50], Step [400/938], D Loss: 0.8499, G Loss: 1.6756, C Loss: -0.5560\n",
      "Epoch [42/50], Step [500/938], D Loss: 0.7964, G Loss: 2.3298, C Loss: -0.3877\n",
      "Epoch [42/50], Step [600/938], D Loss: 0.8229, G Loss: 2.3778, C Loss: -0.3333\n",
      "Epoch [42/50], Step [700/938], D Loss: 0.8183, G Loss: 1.7194, C Loss: -0.4272\n",
      "Epoch [42/50], Step [800/938], D Loss: 0.8710, G Loss: 1.6765, C Loss: -0.4669\n",
      "Epoch [42/50], Step [900/938], D Loss: 0.8339, G Loss: 2.3807, C Loss: -0.2825\n",
      "Epoch [43/50], Step [100/938], D Loss: 0.7662, G Loss: 1.7884, C Loss: -0.4939\n",
      "Epoch [43/50], Step [200/938], D Loss: 0.8631, G Loss: 1.3613, C Loss: -0.5604\n",
      "Epoch [43/50], Step [300/938], D Loss: 0.9693, G Loss: 1.5168, C Loss: -0.5594\n",
      "Epoch [43/50], Step [400/938], D Loss: 0.6209, G Loss: 2.0357, C Loss: -0.3177\n",
      "Epoch [43/50], Step [500/938], D Loss: 0.5492, G Loss: 2.0538, C Loss: -0.3005\n",
      "Epoch [43/50], Step [600/938], D Loss: 0.6525, G Loss: 2.6317, C Loss: -0.1770\n",
      "Epoch [43/50], Step [700/938], D Loss: 0.9840, G Loss: 1.7551, C Loss: -0.3722\n",
      "Epoch [43/50], Step [800/938], D Loss: 0.8735, G Loss: 2.5504, C Loss: -0.1878\n",
      "Epoch [43/50], Step [900/938], D Loss: 0.9026, G Loss: 1.6587, C Loss: -0.4542\n",
      "Epoch [44/50], Step [100/938], D Loss: 0.7540, G Loss: 1.8397, C Loss: -0.4224\n",
      "Epoch [44/50], Step [200/938], D Loss: 0.6444, G Loss: 2.0883, C Loss: -0.2896\n",
      "Epoch [44/50], Step [300/938], D Loss: 0.5425, G Loss: 1.7377, C Loss: -0.3695\n",
      "Epoch [44/50], Step [400/938], D Loss: 0.9615, G Loss: 1.5549, C Loss: -0.4712\n",
      "Epoch [44/50], Step [500/938], D Loss: 0.8239, G Loss: 1.6165, C Loss: -0.4648\n",
      "Epoch [44/50], Step [600/938], D Loss: 0.8657, G Loss: 2.1523, C Loss: -0.4239\n",
      "Epoch [44/50], Step [700/938], D Loss: 0.8059, G Loss: 2.2741, C Loss: -0.2121\n",
      "Epoch [44/50], Step [800/938], D Loss: 0.8342, G Loss: 2.0783, C Loss: -0.2611\n",
      "Epoch [44/50], Step [900/938], D Loss: 0.9542, G Loss: 1.5069, C Loss: -0.5339\n",
      "Epoch [45/50], Step [100/938], D Loss: 0.7553, G Loss: 1.9588, C Loss: -0.3092\n",
      "Epoch [45/50], Step [200/938], D Loss: 0.7651, G Loss: 1.5321, C Loss: -0.5065\n",
      "Epoch [45/50], Step [300/938], D Loss: 0.7019, G Loss: 1.9673, C Loss: -0.3588\n",
      "Epoch [45/50], Step [400/938], D Loss: 0.7259, G Loss: 2.0577, C Loss: -0.3185\n",
      "Epoch [45/50], Step [500/938], D Loss: 1.0550, G Loss: 2.2525, C Loss: -0.3132\n",
      "Epoch [45/50], Step [600/938], D Loss: 0.8886, G Loss: 1.5840, C Loss: -0.4549\n",
      "Epoch [45/50], Step [700/938], D Loss: 0.9165, G Loss: 1.6526, C Loss: -0.4955\n",
      "Epoch [45/50], Step [800/938], D Loss: 0.6957, G Loss: 2.0816, C Loss: -0.2642\n",
      "Epoch [45/50], Step [900/938], D Loss: 0.8061, G Loss: 2.0201, C Loss: -0.2731\n",
      "Epoch [46/50], Step [100/938], D Loss: 0.9340, G Loss: 2.1227, C Loss: -0.3326\n",
      "Epoch [46/50], Step [200/938], D Loss: 0.8613, G Loss: 1.8565, C Loss: -0.3382\n",
      "Epoch [46/50], Step [300/938], D Loss: 0.7551, G Loss: 1.4958, C Loss: -0.4891\n",
      "Epoch [46/50], Step [400/938], D Loss: 0.7973, G Loss: 2.0544, C Loss: -0.2660\n",
      "Epoch [46/50], Step [500/938], D Loss: 0.6737, G Loss: 1.6333, C Loss: -0.3827\n",
      "Epoch [46/50], Step [600/938], D Loss: 0.7352, G Loss: 1.6748, C Loss: -0.4921\n",
      "Epoch [46/50], Step [700/938], D Loss: 1.0847, G Loss: 1.7492, C Loss: -0.4821\n",
      "Epoch [46/50], Step [800/938], D Loss: 0.8187, G Loss: 1.8740, C Loss: -0.3499\n",
      "Epoch [46/50], Step [900/938], D Loss: 0.8948, G Loss: 1.9514, C Loss: -0.3265\n",
      "Epoch [47/50], Step [100/938], D Loss: 0.6328, G Loss: 1.8639, C Loss: -0.3093\n",
      "Epoch [47/50], Step [200/938], D Loss: 0.9561, G Loss: 1.6096, C Loss: -0.4304\n",
      "Epoch [47/50], Step [300/938], D Loss: 1.0736, G Loss: 1.6133, C Loss: -0.4706\n",
      "Epoch [47/50], Step [400/938], D Loss: 1.1851, G Loss: 1.4600, C Loss: -0.5227\n",
      "Epoch [47/50], Step [500/938], D Loss: 0.7411, G Loss: 1.6445, C Loss: -0.4172\n",
      "Epoch [47/50], Step [600/938], D Loss: 0.8492, G Loss: 1.7480, C Loss: -0.4141\n",
      "Epoch [47/50], Step [700/938], D Loss: 0.5922, G Loss: 2.0435, C Loss: -0.3923\n",
      "Epoch [47/50], Step [800/938], D Loss: 0.7433, G Loss: 2.2548, C Loss: -0.2412\n",
      "Epoch [47/50], Step [900/938], D Loss: 0.9896, G Loss: 1.7686, C Loss: -0.4387\n",
      "Epoch [48/50], Step [100/938], D Loss: 0.9624, G Loss: 1.7227, C Loss: -0.3689\n",
      "Epoch [48/50], Step [200/938], D Loss: 0.6691, G Loss: 2.1295, C Loss: -0.2602\n",
      "Epoch [48/50], Step [300/938], D Loss: 0.7849, G Loss: 1.9272, C Loss: -0.4038\n",
      "Epoch [48/50], Step [400/938], D Loss: 1.1063, G Loss: 1.7845, C Loss: -0.4282\n",
      "Epoch [48/50], Step [500/938], D Loss: 0.9734, G Loss: 1.6926, C Loss: -0.4205\n",
      "Epoch [48/50], Step [600/938], D Loss: 0.7996, G Loss: 1.6287, C Loss: -0.4601\n",
      "Epoch [48/50], Step [700/938], D Loss: 1.0891, G Loss: 1.5272, C Loss: -0.4538\n",
      "Epoch [48/50], Step [800/938], D Loss: 0.8148, G Loss: 1.5694, C Loss: -0.4331\n",
      "Epoch [48/50], Step [900/938], D Loss: 0.5557, G Loss: 2.3697, C Loss: -0.2216\n",
      "Epoch [49/50], Step [100/938], D Loss: 0.7105, G Loss: 2.0049, C Loss: -0.2571\n",
      "Epoch [49/50], Step [200/938], D Loss: 0.9314, G Loss: 1.9623, C Loss: -0.3279\n",
      "Epoch [49/50], Step [300/938], D Loss: 0.6770, G Loss: 1.6371, C Loss: -0.4264\n",
      "Epoch [49/50], Step [400/938], D Loss: 0.6994, G Loss: 1.4567, C Loss: -0.4454\n",
      "Epoch [49/50], Step [500/938], D Loss: 0.7222, G Loss: 1.9268, C Loss: -0.3724\n",
      "Epoch [49/50], Step [600/938], D Loss: 0.9306, G Loss: 1.6852, C Loss: -0.4708\n",
      "Epoch [49/50], Step [700/938], D Loss: 0.7817, G Loss: 1.8858, C Loss: -0.3800\n",
      "Epoch [49/50], Step [800/938], D Loss: 0.9221, G Loss: 1.5259, C Loss: -0.4230\n",
      "Epoch [49/50], Step [900/938], D Loss: 0.8609, G Loss: 1.7154, C Loss: -0.4642\n",
      "Epoch [50/50], Step [100/938], D Loss: 0.8762, G Loss: 1.9459, C Loss: -0.3252\n",
      "Epoch [50/50], Step [200/938], D Loss: 0.9219, G Loss: 1.3267, C Loss: -0.5199\n",
      "Epoch [50/50], Step [300/938], D Loss: 1.0525, G Loss: 1.4291, C Loss: -0.5802\n",
      "Epoch [50/50], Step [400/938], D Loss: 0.8252, G Loss: 1.5792, C Loss: -0.4774\n",
      "Epoch [50/50], Step [500/938], D Loss: 1.0285, G Loss: 1.3410, C Loss: -0.5859\n",
      "Epoch [50/50], Step [600/938], D Loss: 0.8803, G Loss: 1.8010, C Loss: -0.4155\n",
      "Epoch [50/50], Step [700/938], D Loss: 0.8015, G Loss: 1.3844, C Loss: -0.5401\n",
      "Epoch [50/50], Step [800/938], D Loss: 0.8363, G Loss: 2.2581, C Loss: -0.2970\n",
      "Epoch [50/50], Step [900/938], D Loss: 0.9768, G Loss: 1.7700, C Loss: -0.4273\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 50\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def difficulty_ramp(difficulty, max_difficulty, growth_rate):\n",
    "    # A logistic growth function to increase difficulty\n",
    "    return max_difficulty / (1 + np.exp(-growth_rate * (difficulty - max_difficulty/2)))\n",
    "\n",
    "C.to(device)\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.view(batch_size, -1).to(device) # flatten\n",
    "\n",
    "        # create labels for real and fake\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # generate random noise + signals\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        signals = torch.randn(batch_size, signal_dim, device=device)\n",
    "        correlated_signals = C(signals)\n",
    "\n",
    "        # use G to get fake images\n",
    "        fake_images = G(noise, correlated_signals.detach())\n",
    "\n",
    "        # D train\n",
    "        D_real = D(real_images, correlated_signals.detach())\n",
    "        D_fake = D(fake_images, correlated_signals.detach())\n",
    "        D_loss_real = adversarial_loss(D_real, real_labels)\n",
    "        D_loss_fake = adversarial_loss(D_fake, fake_labels)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # G train\n",
    "        correlated_signals = C(signals)\n",
    "        fake_images = G(noise, correlated_signals)\n",
    "        D_fake = D(fake_images, correlated_signals)\n",
    "        G_loss = adversarial_loss(D_fake, real_labels)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # C train\n",
    "        # Using assumption that improving G's performance is beneficial for C\n",
    "        # apply difficulty ramp to gradually increase difficulty\n",
    "        difficulty = difficulty_ramp(epoch * len(dataloader) + i, max_difficulty=1.0, growth_rate=0.05)\n",
    "        signals = torch.randn(batch_size, signal_dim, device=device) * difficulty\n",
    "        correlated_signals = C(signals)\n",
    "        fake_images = G(noise, correlated_signals)\n",
    "        D_fake = D(fake_images, correlated_signals)\n",
    "        C_loss = -adversarial_loss(D_fake, fake_labels) \n",
    "\n",
    "        optimizer_C.zero_grad()\n",
    "        C_loss.backward()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "                  f'D Loss: {D_loss.item():.4f}, G Loss: {G_loss.item():.4f}, C Loss: {C_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_generated_images(generator, correlation_device, noise_dim, signal_dim, num_images=10):\n",
    "    noise = torch.randn(num_images, noise_dim, device=device)\n",
    "    signals = torch.randn(num_images, signal_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        correlated_signals = correlation_device(signals)\n",
    "        generated_images = generator(noise, correlated_signals).cpu().view(num_images, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, image in enumerate(generated_images):\n",
    "        plt.subplot(2, num_images//2, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_generated_images(G, C, noise_dim, signal_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-8395",
   "language": "python",
   "name": "cs-8395"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
